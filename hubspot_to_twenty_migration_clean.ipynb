{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "609a5811",
   "metadata": {},
   "source": [
    "## 1. Configuration & Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54bf9d6d",
   "metadata": {},
   "source": [
    "# HubSpot to Twenty CRM - Production Migration Script\n",
    "\n",
    "## ðŸ“‹ Overview\n",
    "Complete migration pipeline for importing HubSpot data (Companies, Contacts, Deals) into Twenty CRM with:\n",
    "- âœ… Automatic database schema preparation (ENUM values)\n",
    "- âœ… Automatic column detection and creation for unmapped HubSpot fields\n",
    "- âœ… Foreign key resolution with deduplication (domain â†’ company, email â†’ person)\n",
    "- âœ… Data type conversion and validation\n",
    "- âœ… Transaction safety with per-row savepoints\n",
    "- âœ… 100% success rate on test data (33/33 records)\n",
    "\n",
    "## ðŸš€ Quick Start\n",
    "1. **Configure Mode**: Set `USE_TEST_DATA = True/False` in Section 1\n",
    "2. **Run Sections 1-3b**: Load data, prepare schema, detect columns\n",
    "3. **Run Section 5**: Execute migration\n",
    "4. **Run Section 6**: Verify results\n",
    "5. **Optional - Section 7**: Purge data for re-migration\n",
    "\n",
    "## ðŸ“Š Migration Statistics (Latest Run)\n",
    "- Companies: 13 âœ…\n",
    "- Contacts: 10 âœ…\n",
    "- Deals: 10 âœ…\n",
    "- Success Rate: 100%\n",
    "- Relationships: 90% contacts linked to companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f5eb9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import hashlib\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cc4b5ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§ª TEST MODE: Using test data files (10 companies, 7 contacts, 10 deals)\n"
     ]
    }
   ],
   "source": [
    "# Database Configuration\n",
    "TWENTY_DB_CONFIG = {\n",
    "    'host': 'twentypsql.postgres.database.azure.com',\n",
    "    'database': 'twenty2',\n",
    "    'user': 'twentyadmin',\n",
    "    'password': 'ISTARIAdmin!',\n",
    "    'port': 5432\n",
    "}\n",
    "\n",
    "WORKSPACE_SCHEMA = 'workspace_bqt42ghwp3i4ag4dk3riyqxco'\n",
    "\n",
    "# Data paths - SWITCH BETWEEN TEST AND FULL\n",
    "USE_TEST_DATA = True  # Set to True for test migration\n",
    "\n",
    "if USE_TEST_DATA:\n",
    "    DATA_DIR = Path('migration_data_test')\n",
    "    print(\"ðŸ§ª TEST MODE: Using test data files (10 companies, 7 contacts, 10 deals)\")\n",
    "else:\n",
    "    DATA_DIR = Path('migration_data')\n",
    "    print(\"ðŸš€ PRODUCTION MODE: Using full data files (33,757 total records)\")\n",
    "\n",
    "CSV_FILES = {\n",
    "    'companies': DATA_DIR / ('test-companies.csv' if USE_TEST_DATA else 'all-companies.csv'),\n",
    "    'contacts': DATA_DIR / ('test-contacts.csv' if USE_TEST_DATA else 'all-contacts.csv'),\n",
    "    'deals': DATA_DIR / ('test-deals.csv' if USE_TEST_DATA else 'all-deals.csv')\n",
    "}\n",
    "\n",
    "def get_db_connection(config):\n",
    "    \"\"\"Create database connection\"\"\"\n",
    "    conn = psycopg2.connect(**config)\n",
    "    print(\"âœ“ Database connection established\")\n",
    "    return conn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a379ad5d",
   "metadata": {},
   "source": [
    "## 2. Load Data & Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0673f8e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading HubSpot data...\n",
      "âœ… Companies: 13 records Ã— 182 columns\n",
      "âœ… Contacts:  10 records Ã— 311 columns\n",
      "âœ… Deals:     10 records Ã— 146 columns\n",
      "\n",
      "ðŸ“Š Total records to migrate: 33\n"
     ]
    }
   ],
   "source": [
    "# Load CSV files\n",
    "print(\"Loading HubSpot data...\")\n",
    "companies_df = pd.read_csv(CSV_FILES['companies'], low_memory=False)\n",
    "contacts_df = pd.read_csv(CSV_FILES['contacts'], low_memory=False)\n",
    "deals_df = pd.read_csv(CSV_FILES['deals'], low_memory=False)\n",
    "\n",
    "print(f\"âœ… Companies: {len(companies_df)} records Ã— {len(companies_df.columns)} columns\")\n",
    "print(f\"âœ… Contacts:  {len(contacts_df)} records Ã— {len(contacts_df.columns)} columns\")\n",
    "print(f\"âœ… Deals:     {len(deals_df)} records Ã— {len(deals_df.columns)} columns\")\n",
    "print(f\"\\nðŸ“Š Total records to migrate: {len(companies_df) + len(contacts_df) + len(deals_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fee192bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Twenty schema loaded:\n",
      "  Company: 204 columns\n",
      "  Person: 327 columns\n",
      "  Opportunity: 154 columns\n"
     ]
    }
   ],
   "source": [
    "# Load Twenty schema (generated from database)\n",
    "with open('twenty_actual_schema.json', 'r') as f:\n",
    "    actual_schema = json.load(f)\n",
    "\n",
    "# Create column name and type mappings\n",
    "company_cols = {col['column_name'] for col in actual_schema['company']}\n",
    "person_cols = {col['column_name'] for col in actual_schema['person']}\n",
    "opportunity_cols = {col['column_name'] for col in actual_schema['opportunity']}\n",
    "\n",
    "company_types = {col['column_name']: col['data_type'] for col in actual_schema['company']}\n",
    "person_types = {col['column_name']: col['data_type'] for col in actual_schema['person']}\n",
    "opportunity_types = {col['column_name']: col['data_type'] for col in actual_schema['opportunity']}\n",
    "\n",
    "print(f\"Twenty schema loaded:\")\n",
    "print(f\"  Company: {len(company_cols)} columns\")\n",
    "print(f\"  Person: {len(person_cols)} columns\")\n",
    "print(f\"  Opportunity: {len(opportunity_cols)} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e694fe38",
   "metadata": {},
   "source": [
    "## 3. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bd84e1d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "UNIQUE DEAL STAGE VALUES IN TEST DATA\n",
      "====================================================================================================\n",
      "\n",
      "Found 4 unique stages:\n",
      "  - Exploration: Meeting scheduled\n",
      "  - Platform Subscribed\n",
      "  - Unfit\n",
      "  - Unfit VIP\n"
     ]
    }
   ],
   "source": [
    "# Check unique Deal Stage values in the data\n",
    "print(\"=\"*100)\n",
    "print(\"UNIQUE DEAL STAGE VALUES IN TEST DATA\")\n",
    "print(\"=\"*100)\n",
    "unique_stages = deals_df['Deal Stage'].dropna().unique()\n",
    "print(f\"\\nFound {len(unique_stages)} unique stages:\")\n",
    "for stage in sorted(unique_stages):\n",
    "    print(f\"  - {stage}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "724814e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "CHECKING CURRENT ENUM VALUES IN DATABASE\n",
      "====================================================================================================\n",
      "âœ“ Database connection established\n",
      "\n",
      "Found ENUM type: opportunity_stage_enum\n",
      "\n",
      "Current values (14):\n",
      "  1.0. OUTREACH_LEADS_TO_CONTACT\n",
      "  2.0. EXPLORATION_MEETING_SCHEDULED\n",
      "  3.0. OFFER_OFFER_REQUESTED\n",
      "  4.0. OFFER_OFFER_SENT\n",
      "  5.0. SIGNED_ONE_OFF\n",
      "  6.0. SIGNED_RECURRING\n",
      "  7.0. LOST_VIP\n",
      "  8.0. LOST_TRASH\n",
      "  9.0. LOST_REGIONS\n",
      "  10.0. OFFER_TENDER_SENT\n",
      "  11.0. PLATFORM_CANCELED\n",
      "  12.0. PLATFORM_SUBSCRIBED\n",
      "  13.0. PROJECT_COMPLETED_AWAITING_FEEDBACK\n",
      "  14.0. PROJECT_PAID\n"
     ]
    }
   ],
   "source": [
    "# Check current ENUM values in database\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"CHECKING CURRENT ENUM VALUES IN DATABASE\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "try:\n",
    "    conn = get_db_connection(TWENTY_DB_CONFIG)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Query to get ENUM type values\n",
    "    cursor.execute(\"\"\"\n",
    "        SELECT\n",
    "            t.typname AS enum_name,\n",
    "            e.enumlabel AS enum_value,\n",
    "            e.enumsortorder\n",
    "        FROM pg_type t\n",
    "        JOIN pg_enum e ON t.oid = e.enumtypid\n",
    "        WHERE t.typname LIKE '%stage%'\n",
    "        ORDER BY t.typname, e.enumsortorder\n",
    "    \"\"\")\n",
    "\n",
    "    results = cursor.fetchall()\n",
    "\n",
    "    if results:\n",
    "        current_enum = results[0][0]\n",
    "        print(f\"\\nFound ENUM type: {current_enum}\")\n",
    "        print(f\"\\nCurrent values ({len(results)}):\")\n",
    "        for enum_name, enum_value, sort_order in results:\n",
    "            print(f\"  {sort_order}. {enum_value}\")\n",
    "    else:\n",
    "        print(\"\\nâš ï¸  No stage-related ENUM types found\")\n",
    "\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ Error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2167b519",
   "metadata": {},
   "source": [
    "## 3a. Database Schema Preparation - Add Missing ENUM Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f7050eeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "PREPARING DATABASE SCHEMA\n",
      "====================================================================================================\n",
      "âœ“ Database connection established\n",
      "Existing ENUM values: 14\n",
      "âœ… All required ENUM values already exist!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def add_missing_enum_values():\n",
    "    \"\"\"Add new ENUM values to opportunity_stage_enum if they don't exist\"\"\"\n",
    "\n",
    "    # Define all stage values we need (based on HubSpot data)\n",
    "    required_stages = {\n",
    "        'OUTREACH_LEADS_TO_CONTACT',\n",
    "        'EXPLORATION_MEETING_SCHEDULED',\n",
    "        'OFFER_OFFER_REQUESTED',\n",
    "        'OFFER_OFFER_SENT',\n",
    "        'OFFER_TENDER_SENT',\n",
    "        'SIGNED_ONE_OFF',\n",
    "        'SIGNED_RECURRING',\n",
    "        'PROJECT_COMPLETED_AWAITING_FEEDBACK',\n",
    "        'PROJECT_PAID',\n",
    "        'PLATFORM_SUBSCRIBED',\n",
    "        'PLATFORM_CANCELED',\n",
    "        'LOST_VIP',\n",
    "        'LOST_REGIONS',\n",
    "        'LOST_TRASH'\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        conn = get_db_connection(TWENTY_DB_CONFIG)\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        # Get existing ENUM values\n",
    "        cursor.execute(\"\"\"\n",
    "            SELECT e.enumlabel\n",
    "            FROM pg_type t\n",
    "            JOIN pg_enum e ON t.oid = e.enumtypid\n",
    "            WHERE t.typname = 'opportunity_stage_enum'\n",
    "        \"\"\")\n",
    "\n",
    "        existing_stages = {row[0] for row in cursor.fetchall()}\n",
    "        print(f\"Existing ENUM values: {len(existing_stages)}\")\n",
    "\n",
    "        # Find missing stages\n",
    "        missing_stages = required_stages - existing_stages\n",
    "\n",
    "        if not missing_stages:\n",
    "            print(\"âœ… All required ENUM values already exist!\")\n",
    "            cursor.close()\n",
    "            conn.close()\n",
    "            return True\n",
    "\n",
    "        print(f\"\\nðŸ“ Adding {len(missing_stages)} new ENUM values:\")\n",
    "\n",
    "        # Add each missing stage\n",
    "        for stage in sorted(missing_stages):\n",
    "            try:\n",
    "                print(f\"  Adding: {stage}...\")\n",
    "                cursor.execute(f\"\"\"\n",
    "                    ALTER TYPE {WORKSPACE_SCHEMA}.opportunity_stage_enum\n",
    "                    ADD VALUE IF NOT EXISTS '{stage}'\n",
    "                \"\"\")\n",
    "                conn.commit()\n",
    "                print(f\"    âœ… Added\")\n",
    "            except Exception as e:\n",
    "                print(f\"    âš ï¸  {str(e)[:100]}\")\n",
    "                conn.rollback()\n",
    "\n",
    "        # Verify all values now exist\n",
    "        cursor.execute(\"\"\"\n",
    "            SELECT e.enumlabel\n",
    "            FROM pg_type t\n",
    "            JOIN pg_enum e ON t.oid = e.enumtypid\n",
    "            WHERE t.typname = 'opportunity_stage_enum'\n",
    "            ORDER BY e.enumsortorder\n",
    "        \"\"\")\n",
    "\n",
    "        final_stages = [row[0] for row in cursor.fetchall()]\n",
    "        print(f\"\\nâœ… Final ENUM values: {len(final_stages)}\")\n",
    "        for i, stage in enumerate(final_stages, 1):\n",
    "            print(f\"  {i}. {stage}\")\n",
    "\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "\n",
    "# Execute the function\n",
    "print(\"=\"*100)\n",
    "print(\"PREPARING DATABASE SCHEMA\")\n",
    "print(\"=\"*100)\n",
    "add_missing_enum_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22266161",
   "metadata": {},
   "source": [
    "## 3b. Column Detection & Schema Sync - Auto-detect Unmapped HubSpot Fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4d3fc388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” SCANNING FOR UNMAPPED HUBSPOT COLUMNS...\n",
      "====================================================================================================\n",
      "DETECTING UNMAPPED HUBSPOT COLUMNS\n",
      "====================================================================================================\n",
      "\n",
      "ðŸ“Š COMPANIES CSV:\n",
      "   Total columns: 182\n",
      "   Unmapped columns: 1\n",
      "     - Company Domain Name â†’ company_domain_name (text) - 13/13 have data\n",
      "\n",
      "ðŸ“Š CONTACTS CSV:\n",
      "   Total columns: 311\n",
      "   Unmapped columns: 1\n",
      "     - LinkedIn URL â†’ linkedin_url (text) - 0/10 have data\n",
      "\n",
      "ðŸ“Š DEALS CSV:\n",
      "   Total columns: 146\n",
      "   Unmapped columns: 0\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "TOTAL UNMAPPED COLUMNS: 2\n",
      "====================================================================================================\n",
      "\n",
      "ðŸ“ ALTER TABLE statements for COMPANY (1 columns):\n",
      "   ALTER TABLE workspace_bqt42ghwp3i4ag4dk3riyqxco.company ADD COLUMN IF NOT EXISTS \"company_domain_name\" text;\n",
      "\n",
      "ðŸ“ ALTER TABLE statements for PERSON (1 columns):\n",
      "   ALTER TABLE workspace_bqt42ghwp3i4ag4dk3riyqxco.person ADD COLUMN IF NOT EXISTS \"linkedin_url\" text;\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "EXECUTING ALTER TABLE STATEMENTS\n",
      "====================================================================================================\n",
      "âœ“ Database connection established\n",
      "\n",
      "âœ… Successfully executed 2/2 ALTER TABLE statements\n",
      "\n",
      "ðŸ“ Updating schema tracking...\n",
      "âœ… Schema tracking updated:\n",
      "   Company: 205 columns\n",
      "   Person: 328 columns\n",
      "   Opportunity: 154 columns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def detect_and_add_unmapped_columns(auto_execute=False):\n",
    "    \"\"\"\n",
    "    Detect unmapped columns from HubSpot CSV and optionally add them to database\n",
    "\n",
    "    Args:\n",
    "        auto_execute: If True, automatically execute ALTER TABLE statements\n",
    "                      If False, only preview the changes\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"=\"*100)\n",
    "    print(\"DETECTING UNMAPPED HUBSPOT COLUMNS\")\n",
    "    print(\"=\"*100)\n",
    "\n",
    "    # Define column mappings for normalization\n",
    "    def normalize_column_name(hubspot_col):\n",
    "        \"\"\"Convert HubSpot column name to PostgreSQL column name\"\"\"\n",
    "        pg_col = hubspot_col.lower().replace(' ', '_').replace('/', '_').replace('-', '_')\n",
    "        pg_col = ''.join(c for c in pg_col if c.isalnum() or c == '_')\n",
    "        return pg_col\n",
    "\n",
    "    def infer_pg_type(series):\n",
    "        \"\"\"Infer PostgreSQL type from pandas Series\"\"\"\n",
    "        # Skip if all null\n",
    "        if series.isna().all():\n",
    "            return 'text'\n",
    "\n",
    "        # Get non-null values\n",
    "        non_null = series.dropna()\n",
    "\n",
    "        # Try to determine type\n",
    "        if pd.api.types.is_integer_dtype(non_null):\n",
    "            return 'bigint'\n",
    "        elif pd.api.types.is_float_dtype(non_null):\n",
    "            return 'double precision'\n",
    "        elif pd.api.types.is_bool_dtype(non_null):\n",
    "            return 'boolean'\n",
    "        elif pd.api.types.is_datetime64_any_dtype(non_null):\n",
    "            return 'timestamp with time zone'\n",
    "        else:\n",
    "            # Check string length for text vs varchar\n",
    "            if non_null.astype(str).str.len().max() > 255:\n",
    "                return 'text'\n",
    "            else:\n",
    "                return 'text'  # Use text for flexibility\n",
    "\n",
    "    unmapped_columns = {\n",
    "        'company': [],\n",
    "        'person': [],\n",
    "        'opportunity': []\n",
    "    }\n",
    "\n",
    "    # Skip columns that are already handled or internal\n",
    "    skip_columns = {\n",
    "        'Record ID', 'Create Date', 'Last Modified Date',\n",
    "        'Company name', 'Website URL', 'Number of Employees', 'LinkedIn Company Page',\n",
    "        'City', 'State/Region', 'Country/Region', 'Postal Code', 'Street Address',\n",
    "        'First Name', 'Last Name', 'Email', 'Phone Number', 'Job Title', 'LinkedIn Profile',\n",
    "        'Associated Company IDs', 'Associated Company IDs (Primary)',\n",
    "        'Associated Contact IDs', 'Associated Contact IDs (Primary)',\n",
    "        'Deal Name', 'Amount', 'Close Date', 'Deal Stage', 'Deal Currency Code'\n",
    "    }\n",
    "\n",
    "    # Check companies CSV\n",
    "    print(\"\\nðŸ“Š COMPANIES CSV:\")\n",
    "    print(f\"   Total columns: {len(companies_df.columns)}\")\n",
    "    for hubspot_col in companies_df.columns:\n",
    "        if hubspot_col in skip_columns:\n",
    "            continue\n",
    "\n",
    "        pg_col = normalize_column_name(hubspot_col)\n",
    "        if pg_col not in company_cols:\n",
    "            pg_type = infer_pg_type(companies_df[hubspot_col])\n",
    "            unmapped_columns['company'].append({\n",
    "                'hubspot_name': hubspot_col,\n",
    "                'pg_name': pg_col,\n",
    "                'pg_type': pg_type,\n",
    "                'non_null_count': companies_df[hubspot_col].notna().sum(),\n",
    "                'total_count': len(companies_df)\n",
    "            })\n",
    "\n",
    "    print(f\"   Unmapped columns: {len(unmapped_columns['company'])}\")\n",
    "    for col in unmapped_columns['company'][:5]:  # Show first 5\n",
    "        print(f\"     - {col['hubspot_name']} â†’ {col['pg_name']} ({col['pg_type']}) - {col['non_null_count']}/{col['total_count']} have data\")\n",
    "    if len(unmapped_columns['company']) > 5:\n",
    "        print(f\"     ... and {len(unmapped_columns['company']) - 5} more\")\n",
    "\n",
    "    # Check contacts CSV\n",
    "    print(\"\\nðŸ“Š CONTACTS CSV:\")\n",
    "    print(f\"   Total columns: {len(contacts_df.columns)}\")\n",
    "    for hubspot_col in contacts_df.columns:\n",
    "        if hubspot_col in skip_columns:\n",
    "            continue\n",
    "\n",
    "        pg_col = normalize_column_name(hubspot_col)\n",
    "        if pg_col not in person_cols:\n",
    "            pg_type = infer_pg_type(contacts_df[hubspot_col])\n",
    "            unmapped_columns['person'].append({\n",
    "                'hubspot_name': hubspot_col,\n",
    "                'pg_name': pg_col,\n",
    "                'pg_type': pg_type,\n",
    "                'non_null_count': contacts_df[hubspot_col].notna().sum(),\n",
    "                'total_count': len(contacts_df)\n",
    "            })\n",
    "\n",
    "    print(f\"   Unmapped columns: {len(unmapped_columns['person'])}\")\n",
    "    for col in unmapped_columns['person'][:5]:\n",
    "        print(f\"     - {col['hubspot_name']} â†’ {col['pg_name']} ({col['pg_type']}) - {col['non_null_count']}/{col['total_count']} have data\")\n",
    "    if len(unmapped_columns['person']) > 5:\n",
    "        print(f\"     ... and {len(unmapped_columns['person']) - 5} more\")\n",
    "\n",
    "    # Check deals CSV\n",
    "    print(\"\\nðŸ“Š DEALS CSV:\")\n",
    "    print(f\"   Total columns: {len(deals_df.columns)}\")\n",
    "    for hubspot_col in deals_df.columns:\n",
    "        if hubspot_col in skip_columns:\n",
    "            continue\n",
    "\n",
    "        pg_col = normalize_column_name(hubspot_col)\n",
    "        if pg_col not in opportunity_cols:\n",
    "            pg_type = infer_pg_type(deals_df[hubspot_col])\n",
    "            unmapped_columns['opportunity'].append({\n",
    "                'hubspot_name': hubspot_col,\n",
    "                'pg_name': pg_col,\n",
    "                'pg_type': pg_type,\n",
    "                'non_null_count': deals_df[hubspot_col].notna().sum(),\n",
    "                'total_count': len(deals_df)\n",
    "            })\n",
    "\n",
    "    print(f\"   Unmapped columns: {len(unmapped_columns['opportunity'])}\")\n",
    "    for col in unmapped_columns['opportunity'][:5]:\n",
    "        print(f\"     - {col['hubspot_name']} â†’ {col['pg_name']} ({col['pg_type']}) - {col['non_null_count']}/{col['total_count']} have data\")\n",
    "    if len(unmapped_columns['opportunity']) > 5:\n",
    "        print(f\"     ... and {len(unmapped_columns['opportunity']) - 5} more\")\n",
    "\n",
    "    # Generate ALTER TABLE statements\n",
    "    total_unmapped = len(unmapped_columns['company']) + len(unmapped_columns['person']) + len(unmapped_columns['opportunity'])\n",
    "\n",
    "    if total_unmapped == 0:\n",
    "        print(\"\\nâœ… All HubSpot columns are already mapped to database schema!\")\n",
    "        return True\n",
    "\n",
    "    print(f\"\\n\\n{'='*100}\")\n",
    "    print(f\"TOTAL UNMAPPED COLUMNS: {total_unmapped}\")\n",
    "    print(f\"{'='*100}\")\n",
    "\n",
    "    alter_statements = []\n",
    "\n",
    "    # Generate ALTER TABLE for company\n",
    "    if unmapped_columns['company']:\n",
    "        print(f\"\\nðŸ“ ALTER TABLE statements for COMPANY ({len(unmapped_columns['company'])} columns):\")\n",
    "        for col in unmapped_columns['company']:\n",
    "            stmt = f'ALTER TABLE {WORKSPACE_SCHEMA}.company ADD COLUMN IF NOT EXISTS \"{col[\"pg_name\"]}\" {col[\"pg_type\"]};'\n",
    "            alter_statements.append(('company', stmt))\n",
    "            print(f\"   {stmt}\")\n",
    "\n",
    "    # Generate ALTER TABLE for person\n",
    "    if unmapped_columns['person']:\n",
    "        print(f\"\\nðŸ“ ALTER TABLE statements for PERSON ({len(unmapped_columns['person'])} columns):\")\n",
    "        for col in unmapped_columns['person']:\n",
    "            stmt = f'ALTER TABLE {WORKSPACE_SCHEMA}.person ADD COLUMN IF NOT EXISTS \"{col[\"pg_name\"]}\" {col[\"pg_type\"]};'\n",
    "            alter_statements.append(('person', stmt))\n",
    "            print(f\"   {stmt}\")\n",
    "\n",
    "    # Generate ALTER TABLE for opportunity\n",
    "    if unmapped_columns['opportunity']:\n",
    "        print(f\"\\nðŸ“ ALTER TABLE statements for OPPORTUNITY ({len(unmapped_columns['opportunity'])} columns):\")\n",
    "        for col in unmapped_columns['opportunity']:\n",
    "            stmt = f'ALTER TABLE {WORKSPACE_SCHEMA}.opportunity ADD COLUMN IF NOT EXISTS \"{col[\"pg_name\"]}\" {col[\"pg_type\"]};'\n",
    "            alter_statements.append(('opportunity', stmt))\n",
    "            print(f\"   {stmt}\")\n",
    "\n",
    "    # Execute if requested\n",
    "    if auto_execute:\n",
    "        print(f\"\\n\\n{'='*100}\")\n",
    "        print(\"EXECUTING ALTER TABLE STATEMENTS\")\n",
    "        print(f\"{'='*100}\")\n",
    "\n",
    "        try:\n",
    "            conn = get_db_connection(TWENTY_DB_CONFIG)\n",
    "            cursor = conn.cursor()\n",
    "\n",
    "            success_count = 0\n",
    "            for table, stmt in alter_statements:\n",
    "                try:\n",
    "                    cursor.execute(stmt)\n",
    "                    conn.commit()\n",
    "                    success_count += 1\n",
    "                except Exception as e:\n",
    "                    print(f\"  âš ï¸  Error on {table}: {str(e)[:100]}\")\n",
    "                    conn.rollback()\n",
    "\n",
    "            print(f\"\\nâœ… Successfully executed {success_count}/{len(alter_statements)} ALTER TABLE statements\")\n",
    "\n",
    "            # Update schema tracking in memory\n",
    "            print(\"\\nðŸ“ Updating schema tracking...\")\n",
    "            for table, col_info in [('company', unmapped_columns['company']),\n",
    "                                     ('person', unmapped_columns['person']),\n",
    "                                     ('opportunity', unmapped_columns['opportunity'])]:\n",
    "                for col in col_info:\n",
    "                    if table == 'company':\n",
    "                        company_cols.add(col['pg_name'])\n",
    "                        company_types[col['pg_name']] = col['pg_type']\n",
    "                    elif table == 'person':\n",
    "                        person_cols.add(col['pg_name'])\n",
    "                        person_types[col['pg_name']] = col['pg_type']\n",
    "                    elif table == 'opportunity':\n",
    "                        opportunity_cols.add(col['pg_name'])\n",
    "                        opportunity_types[col['pg_name']] = col['pg_type']\n",
    "\n",
    "            print(f\"âœ… Schema tracking updated:\")\n",
    "            print(f\"   Company: {len(company_cols)} columns\")\n",
    "            print(f\"   Person: {len(person_cols)} columns\")\n",
    "            print(f\"   Opportunity: {len(opportunity_cols)} columns\")\n",
    "\n",
    "            cursor.close()\n",
    "            conn.close()\n",
    "\n",
    "            return True\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\nâŒ Error: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return False\n",
    "    else:\n",
    "        print(f\"\\n\\nâš ï¸  PREVIEW MODE: Set auto_execute=True to add these columns to the database\")\n",
    "        return False\n",
    "\n",
    "# Execute column detection\n",
    "print(\"\\nðŸ” SCANNING FOR UNMAPPED HUBSPOT COLUMNS...\")\n",
    "detect_and_add_unmapped_columns(auto_execute=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec44b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Helper functions loaded\n"
     ]
    }
   ],
   "source": [
    "def generate_uuid_from_hubspot_id(hubspot_id):\n",
    "    \"\"\"Generate consistent UUID from HubSpot Record ID\"\"\"\n",
    "    namespace = uuid.UUID('6ba7b810-9dad-11d1-80b4-00c04fd430c8')\n",
    "    return str(uuid.uuid5(namespace, str(hubspot_id)))\n",
    "\n",
    "def parse_timestamp(timestamp_str):\n",
    "    \"\"\"Parse HubSpot timestamp to ISO format\"\"\"\n",
    "    if pd.isna(timestamp_str):\n",
    "        return None\n",
    "    try:\n",
    "        dt = pd.to_datetime(timestamp_str)\n",
    "        return dt.isoformat() + 'Z' if dt.tzinfo is None else dt.isoformat()\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def map_deal_stage(hubspot_stage):\n",
    "    \"\"\"Map HubSpot deal stages to Twenty ENUM stage values\"\"\"\n",
    "    stage_str = str(hubspot_stage)\n",
    "    stage_lower = stage_str.lower()\n",
    "\n",
    "    # Exact mapping: HubSpot stage â†’ Twenty ENUM value\n",
    "    stage_mapping = {\n",
    "        'outreach: leads to contact': 'OUTREACH_LEADS_TO_CONTACT',\n",
    "        'exploration: meeting scheduled': 'EXPLORATION_MEETING_SCHEDULED',\n",
    "        'exploration: offer requested': 'OFFER_OFFER_REQUESTED',\n",
    "        'exploration: offer sent': 'OFFER_OFFER_SENT',\n",
    "        'exploration: tender sent': 'OFFER_OFFER_SENT',  # Maps to same as \"Offer Sent\"\n",
    "        'project signed': 'SIGNED_ONE_OFF',\n",
    "        'project completed: awaiting feedback': 'SIGNED_ONE_OFF',\n",
    "        'project paid': 'SIGNED_ONE_OFF',\n",
    "        'platform subscribed': 'SIGNED_RECURRING',\n",
    "        'platform canceled': 'LOST_VIP',\n",
    "        'unfit vip': 'LOST_VIP',\n",
    "        'unfit regions': 'LOST_TRASH',\n",
    "        'unfit': 'LOST_TRASH'\n",
    "    }\n",
    "\n",
    "    # Check exact match first (case-insensitive)\n",
    "    if stage_lower in stage_mapping:\n",
    "        return stage_mapping[stage_lower]\n",
    "\n",
    "    # Fallback to default stage for unmapped values\n",
    "    print(f\"âš ï¸  Unmapped stage '{hubspot_stage}' â†’ defaulting to OUTREACH_LEADS_TO_CONTACT\")\n",
    "    return 'OUTREACH_LEADS_TO_CONTACT'\n",
    "\n",
    "def convert_value_to_target_type(value, target_type):\n",
    "    \"\"\"Convert value to match PostgreSQL data type\"\"\"\n",
    "    if value is None or (isinstance(value, str) and value.strip() == ''):\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        if target_type in ['numeric', 'double precision']:\n",
    "            if isinstance(value, str):\n",
    "                if value.lower() in ['true', 't', 'yes', 'y']:\n",
    "                    return 1\n",
    "                elif value.lower() in ['false', 'f', 'no', 'n']:\n",
    "                    return 0\n",
    "            return float(value)\n",
    "\n",
    "        elif target_type in ['integer', 'bigint', 'smallint']:\n",
    "            if isinstance(value, str):\n",
    "                if value.lower() in ['true', 't', 'yes', 'y']:\n",
    "                    return 1\n",
    "                elif value.lower() in ['false', 'f', 'no', 'n']:\n",
    "                    return 0\n",
    "            int_value = int(float(value))\n",
    "            # Skip values that would overflow PostgreSQL integer (32-bit)\n",
    "            if target_type == 'integer' and abs(int_value) > 2147483647:\n",
    "                return None\n",
    "            return int_value\n",
    "\n",
    "        elif target_type == 'boolean':\n",
    "            if isinstance(value, bool):\n",
    "                return value\n",
    "            if isinstance(value, str):\n",
    "                return value.lower() in ['true', 't', 'yes', 'y', '1']\n",
    "            return bool(value)\n",
    "\n",
    "        elif target_type in ['text', 'character varying', 'varchar']:\n",
    "            return str(value)\n",
    "\n",
    "        else:\n",
    "            return value\n",
    "\n",
    "    except (ValueError, TypeError):\n",
    "        return None\n",
    "\n",
    "print(\"âœ… Helper functions loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5cb1cc2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "TESTING UPDATED STAGE MAPPING\n",
      "====================================================================================================\n",
      "\n",
      "Mapping 4 unique stages from test data:\n",
      "\n",
      "  'Exploration: Meeting scheduled' â†’ EXPLORATION_MEETING_SCHEDULED\n",
      "  'Platform Subscribed' â†’ SIGNED_RECURRING\n",
      "  'Unfit' â†’ LOST_TRASH\n",
      "  'Unfit VIP' â†’ LOST_VIP\n"
     ]
    }
   ],
   "source": [
    "# Test the updated mapping with actual data\n",
    "print(\"=\"*100)\n",
    "print(\"TESTING UPDATED STAGE MAPPING\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "test_stages = deals_df['Deal Stage'].dropna().unique()\n",
    "print(f\"\\nMapping {len(test_stages)} unique stages from test data:\\n\")\n",
    "\n",
    "for stage in sorted(test_stages):\n",
    "    mapped = map_deal_stage(stage)\n",
    "    print(f\"  '{stage}' â†’ {mapped}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25a52b0",
   "metadata": {},
   "source": [
    "## 4. Data Transformation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8223b9a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Data transformation functions loaded (with FK resolution)\n"
     ]
    }
   ],
   "source": [
    "# Track inserted IDs for foreign key validation and cleanup\n",
    "inserted_company_ids = set()\n",
    "inserted_person_ids = set()\n",
    "\n",
    "# NEW: FK resolution helpers - these will be populated during migration\n",
    "domain_to_company_id = {}  # Maps domain â†’ actual company.id from DB\n",
    "email_to_person_id = {}     # Maps email â†’ actual person.id from DB\n",
    "hubspot_company_to_domain = {}  # Maps HubSpot company Record ID â†’ domain\n",
    "hubspot_contact_to_email = {}   # Maps HubSpot contact Record ID â†’ email\n",
    "\n",
    "def build_company_insert_data(row):\n",
    "    \"\"\"Build company insert data with type conversion\"\"\"\n",
    "    data = {}\n",
    "\n",
    "    # Core fields\n",
    "    data['id'] = generate_uuid_from_hubspot_id(str(row['Record ID']))\n",
    "    data['name'] = str(row.get('Company name', '')) if pd.notna(row.get('Company name')) else None\n",
    "\n",
    "    if pd.notna(row.get('Website URL')):\n",
    "        domain = str(row['Website URL'])\n",
    "        data['domainNamePrimaryLinkUrl'] = domain\n",
    "        # Track mapping for FK resolution\n",
    "        hubspot_company_to_domain[str(row['Record ID'])] = domain\n",
    "\n",
    "    if pd.notna(row.get('Number of Employees')):\n",
    "        data['employees'] = convert_value_to_target_type(row['Number of Employees'], 'double precision')\n",
    "\n",
    "    if pd.notna(row.get('LinkedIn Company Page')):\n",
    "        data['linkedinLinkPrimaryLinkUrl'] = str(row['LinkedIn Company Page'])\n",
    "\n",
    "    # Address fields\n",
    "    if pd.notna(row.get('City')):\n",
    "        data['addressAddressCity'] = str(row['City'])\n",
    "    if pd.notna(row.get('State/Region')):\n",
    "        data['addressAddressState'] = str(row['State/Region'])\n",
    "    if pd.notna(row.get('Country/Region')):\n",
    "        data['addressAddressCountry'] = str(row['Country/Region'])\n",
    "    if pd.notna(row.get('Postal Code')):\n",
    "        data['addressAddressPostcode'] = str(row['Postal Code'])\n",
    "    if pd.notna(row.get('Street Address')):\n",
    "        data['addressAddressStreet1'] = str(row['Street Address'])\n",
    "\n",
    "    # Timestamps\n",
    "    if pd.notna(row.get('Create Date')):\n",
    "        data['createdAt'] = parse_timestamp(row['Create Date'])\n",
    "    else:\n",
    "        data['createdAt'] = datetime.utcnow().isoformat() + 'Z'\n",
    "\n",
    "    if pd.notna(row.get('Last Modified Date')):\n",
    "        data['updatedAt'] = parse_timestamp(row['Last Modified Date'])\n",
    "    else:\n",
    "        data['updatedAt'] = data['createdAt']\n",
    "\n",
    "    # Map remaining HubSpot columns\n",
    "    skip_cols = ['Record ID', 'Company name', 'Website URL', 'Number of Employees',\n",
    "                 'LinkedIn Company Page', 'City', 'State/Region', 'Country/Region',\n",
    "                 'Postal Code', 'Street Address', 'Create Date', 'Last Modified Date']\n",
    "\n",
    "    for hubspot_col in companies_df.columns:\n",
    "        if hubspot_col in skip_cols:\n",
    "            continue\n",
    "\n",
    "        pg_col_name = hubspot_col.lower().replace(' ', '_').replace('/', '_').replace('-', '_')\n",
    "        pg_col_name = ''.join(c for c in pg_col_name if c.isalnum() or c == '_')\n",
    "\n",
    "        if pg_col_name in company_cols and pd.notna(row.get(hubspot_col)):\n",
    "            target_type = company_types.get(pg_col_name, 'text')\n",
    "            converted_val = convert_value_to_target_type(row[hubspot_col], target_type)\n",
    "            if converted_val is not None:\n",
    "                data[pg_col_name] = converted_val\n",
    "\n",
    "    return data\n",
    "\n",
    "def build_person_insert_data(row, cursor):\n",
    "    \"\"\"Build person insert data with DB-based FK resolution\"\"\"\n",
    "    data = {}\n",
    "\n",
    "    data['id'] = generate_uuid_from_hubspot_id(str(row['Record ID']))\n",
    "\n",
    "    if pd.notna(row.get('First Name')):\n",
    "        data['nameFirstName'] = str(row['First Name'])\n",
    "    if pd.notna(row.get('Last Name')):\n",
    "        data['nameLastName'] = str(row['Last Name'])\n",
    "\n",
    "    if pd.notna(row.get('Email')):\n",
    "        email = str(row['Email'])\n",
    "        data['emailsPrimaryEmail'] = email\n",
    "        # Track mapping for FK resolution in opportunities\n",
    "        hubspot_contact_to_email[str(row['Record ID'])] = email\n",
    "\n",
    "    if pd.notna(row.get('Phone Number')):\n",
    "        data['phonesPrimaryPhoneNumber'] = str(row['Phone Number'])\n",
    "\n",
    "    if pd.notna(row.get('Job Title')):\n",
    "        data['jobTitle'] = str(row['Job Title'])\n",
    "\n",
    "    if pd.notna(row.get('LinkedIn Profile')):\n",
    "        data['linkedinLinkPrimaryLinkUrl'] = str(row['LinkedIn Profile'])\n",
    "\n",
    "    if pd.notna(row.get('City')):\n",
    "        data['city'] = str(row['City'])\n",
    "\n",
    "    # Company relationship - RESOLVE BY DOMAIN (primary strategy)\n",
    "    # Try both column name variations\n",
    "    company_col = 'Associated Company IDs (Primary)' if 'Associated Company IDs (Primary)' in contacts_df.columns else 'Associated Company IDs'\n",
    "    if pd.notna(row.get(company_col)):\n",
    "        company_ids = str(row[company_col]).split(';')\n",
    "        if company_ids:\n",
    "            # Normalize ID: remove .0 from float representation\n",
    "            company_hubspot_id = company_ids[0].strip().replace('.0', '')\n",
    "\n",
    "            # Try to get domain from our mapping\n",
    "            domain = hubspot_company_to_domain.get(company_hubspot_id)\n",
    "\n",
    "            if domain and domain in domain_to_company_id:\n",
    "                # Use actual company ID from DB (handles deduplication)\n",
    "                data['companyId'] = domain_to_company_id[domain]\n",
    "            else:\n",
    "                # Fallback: query DB for company by domain if we have the Record ID in companies CSV\n",
    "                try:\n",
    "                    matching_company = companies_df[companies_df['Record ID'].astype(str) == company_hubspot_id]\n",
    "                    if not matching_company.empty and pd.notna(matching_company.iloc[0].get('Website URL')):\n",
    "                        lookup_domain = str(matching_company.iloc[0]['Website URL'])\n",
    "                        cursor.execute(\n",
    "                            f'SELECT id FROM {WORKSPACE_SCHEMA}.company WHERE \"domainNamePrimaryLinkUrl\" = %s LIMIT 1',\n",
    "                            (lookup_domain,)\n",
    "                        )\n",
    "                        result = cursor.fetchone()\n",
    "                        if result:\n",
    "                            data['companyId'] = result[0]\n",
    "                except:\n",
    "                    pass  # Skip if lookup fails\n",
    "\n",
    "    # Timestamps\n",
    "    if pd.notna(row.get('Create Date')):\n",
    "        data['createdAt'] = parse_timestamp(row['Create Date'])\n",
    "    else:\n",
    "        data['createdAt'] = datetime.utcnow().isoformat() + 'Z'\n",
    "\n",
    "    if pd.notna(row.get('Last Modified Date')):\n",
    "        data['updatedAt'] = parse_timestamp(row['Last Modified Date'])\n",
    "    else:\n",
    "        data['updatedAt'] = data['createdAt']\n",
    "\n",
    "    skip_cols = ['Record ID', 'First Name', 'Last Name', 'Email', 'Phone Number',\n",
    "                 'Job Title', 'LinkedIn Profile', 'City', 'Associated Company IDs', 'Associated Company IDs (Primary)',\n",
    "                 'Create Date', 'Last Modified Date']\n",
    "\n",
    "    for hubspot_col in contacts_df.columns:\n",
    "        if hubspot_col in skip_cols:\n",
    "            continue\n",
    "\n",
    "        pg_col_name = hubspot_col.lower().replace(' ', '_').replace('/', '_').replace('-', '_')\n",
    "        pg_col_name = ''.join(c for c in pg_col_name if c.isalnum() or c == '_')\n",
    "\n",
    "        if pg_col_name in person_cols and pd.notna(row.get(hubspot_col)):\n",
    "            target_type = person_types.get(pg_col_name, 'text')\n",
    "            converted_val = convert_value_to_target_type(row[hubspot_col], target_type)\n",
    "            if converted_val is not None:\n",
    "                data[pg_col_name] = converted_val\n",
    "\n",
    "    return data\n",
    "\n",
    "def build_opportunity_insert_data(row, cursor):\n",
    "    \"\"\"Build opportunity insert data with DB-based FK resolution\"\"\"\n",
    "    data = {}\n",
    "\n",
    "    data['id'] = generate_uuid_from_hubspot_id(str(row['Record ID']))\n",
    "    data['name'] = str(row.get('Deal Name', '')) if pd.notna(row.get('Deal Name')) else None\n",
    "\n",
    "    # Amount\n",
    "    if pd.notna(row.get('Amount')):\n",
    "        try:\n",
    "            amount = float(row['Amount'])\n",
    "            data['amountAmountMicros'] = int(amount * 1000000)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    if pd.notna(row.get('Deal Currency Code')):\n",
    "        data['amountCurrencyCode'] = str(row['Deal Currency Code'])\n",
    "    else:\n",
    "        data['amountCurrencyCode'] = 'USD'\n",
    "\n",
    "    if pd.notna(row.get('Close Date')):\n",
    "        data['closeDate'] = parse_timestamp(row['Close Date'])\n",
    "\n",
    "    if pd.notna(row.get('Deal Stage')):\n",
    "        data['stage'] = map_deal_stage(str(row['Deal Stage']))\n",
    "    else:\n",
    "        data['stage'] = 'NEW'\n",
    "    # Try both column name variations\n",
    "    company_col = 'Associated Company IDs (Primary)' if 'Associated Company IDs (Primary)' in deals_df.columns else 'Associated Company IDs'\n",
    "    if pd.notna(row.get(company_col)):\n",
    "        company_ids = str(row[company_col]).split(';')\n",
    "        if company_ids:\n",
    "            # Normalize ID: remove .0 from float representation\n",
    "            company_hubspot_id = company_ids[0].strip().replace('.0', '')\n",
    "\n",
    "            # Try to get domain from our mapping\n",
    "            domain = hubspot_company_to_domain.get(company_hubspot_id)\n",
    "\n",
    "            if domain and domain in domain_to_company_id:\n",
    "                # Use actual company ID from DB (handles deduplication)\n",
    "                data['companyId'] = domain_to_company_id[domain]\n",
    "            else:\n",
    "                # Fallback: query DB for company by domain\n",
    "                try:\n",
    "                    matching_company = companies_df[companies_df['Record ID'].astype(str) == company_hubspot_id]\n",
    "                    if not matching_company.empty and pd.notna(matching_company.iloc[0].get('Website URL')):\n",
    "                        lookup_domain = str(matching_company.iloc[0]['Website URL'])\n",
    "                        cursor.execute(\n",
    "                            f'SELECT id FROM {WORKSPACE_SCHEMA}.company WHERE \"domainNamePrimaryLinkUrl\" = %s LIMIT 1',\n",
    "                            (lookup_domain,)\n",
    "                        )\n",
    "                        result = cursor.fetchone()\n",
    "                        if result:\n",
    "                            data['companyId'] = result[0]\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "    # Contact relationship - RESOLVE BY EMAIL (primary strategy)\n",
    "    if pd.notna(row.get('Associated Contact IDs')):\n",
    "        contact_ids = str(row['Associated Contact IDs']).split(';')\n",
    "        if contact_ids:\n",
    "            # Normalize ID: remove .0 from float representation\n",
    "            contact_hubspot_id = contact_ids[0].strip().replace('.0', '')\n",
    "\n",
    "            # Try to get email from our mapping\n",
    "            email = hubspot_contact_to_email.get(contact_hubspot_id)\n",
    "\n",
    "            if email and email in email_to_person_id:\n",
    "                # Use actual person ID from DB (handles deduplication)\n",
    "                data['pointOfContactId'] = email_to_person_id[email]\n",
    "            else:\n",
    "                # Fallback: query DB for person by email\n",
    "                try:\n",
    "                    matching_contact = contacts_df[contacts_df['Record ID'].astype(str) == contact_hubspot_id]\n",
    "                    if not matching_contact.empty and pd.notna(matching_contact.iloc[0].get('Email')):\n",
    "                        lookup_email = str(matching_contact.iloc[0]['Email'])\n",
    "                        cursor.execute(\n",
    "                            f'SELECT id FROM {WORKSPACE_SCHEMA}.person WHERE \"emailsPrimaryEmail\" = %s LIMIT 1',\n",
    "                            (lookup_email,)\n",
    "                        )\n",
    "                        result = cursor.fetchone()\n",
    "                        if result:\n",
    "                            data['pointOfContactId'] = result[0]\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "    # Timestamps\n",
    "    if pd.notna(row.get('Create Date')):\n",
    "        data['createdAt'] = parse_timestamp(row['Create Date'])\n",
    "    else:\n",
    "        data['createdAt'] = datetime.utcnow().isoformat() + 'Z'\n",
    "\n",
    "    if pd.notna(row.get('Last Modified Date')):\n",
    "        data['updatedAt'] = parse_timestamp(row['Last Modified Date'])\n",
    "    else:\n",
    "        data['updatedAt'] = data['createdAt']\n",
    "\n",
    "    skip_cols = ['Record ID', 'Deal Name', 'Amount', 'Close Date',\n",
    "                 'Deal Stage', 'Associated Company IDs', 'Associated Company IDs (Primary)',\n",
    "                 'Associated Contact IDs', 'Associated Contact IDs (Primary)',\n",
    "                 'Create Date', 'Last Modified Date']\n",
    "\n",
    "    for hubspot_col in deals_df.columns:\n",
    "        if hubspot_col in skip_cols:\n",
    "            continue\n",
    "\n",
    "        pg_col_name = hubspot_col.lower().replace(' ', '_').replace('/', '_').replace('-', '_')\n",
    "        pg_col_name = ''.join(c for c in pg_col_name if c.isalnum() or c == '_')\n",
    "\n",
    "        if pg_col_name in opportunity_cols and pd.notna(row.get(hubspot_col)):\n",
    "            target_type = opportunity_types.get(pg_col_name, 'text')\n",
    "            converted_val = convert_value_to_target_type(row[hubspot_col], target_type)\n",
    "            if converted_val is not None:\n",
    "                data[pg_col_name] = converted_val\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "print(\"âœ… Data transformation functions loaded (with FK resolution)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1dabf8",
   "metadata": {},
   "source": [
    "## 5. Execute Migration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8cb0dede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ STARTING MIGRATION...\n",
      "âœ“ Database connection established\n",
      "\n",
      "====================================================================================================\n",
      "PHASE 1: MIGRATING COMPANIES\n",
      "====================================================================================================\n",
      "Records to migrate: 13\n",
      "\n",
      "\n",
      "âœ… Companies: 13 migrated, 0 failed\n",
      "   FK Resolution Map: 13 domains â†’ company IDs\n",
      "\n",
      "====================================================================================================\n",
      "PHASE 2: MIGRATING CONTACTS\n",
      "====================================================================================================\n",
      "Records to migrate: 10\n",
      "\n",
      "\n",
      "âœ… Persons: 10 migrated, 0 failed\n",
      "   FK Resolution Map: 10 emails â†’ person IDs\n",
      "\n",
      "====================================================================================================\n",
      "PHASE 3: MIGRATING DEALS\n",
      "====================================================================================================\n",
      "Records to migrate: 10\n",
      "\n",
      "\n",
      "âœ… Opportunities: 10 migrated, 0 failed\n",
      "\n",
      "====================================================================================================\n",
      "ðŸŽ‰ MIGRATION COMPLETE!\n",
      "====================================================================================================\n",
      "\n",
      "SUMMARY:\n",
      "  Companies:        13 âœ…      0 âŒ\n",
      "  Persons:          10 âœ…      0 âŒ\n",
      "  Opportunities:    10 âœ…      0 âŒ\n",
      "\n",
      "  TOTAL:            33 âœ…      0 âŒ\n",
      "  Success Rate:  100.0%\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "def execute_migration():\n",
    "    \"\"\"Execute complete migration with transaction safety, per-row savepoints, and FK resolution\"\"\"\n",
    "    global inserted_company_ids, inserted_person_ids\n",
    "    global domain_to_company_id, email_to_person_id\n",
    "    global hubspot_company_to_domain, hubspot_contact_to_email\n",
    "\n",
    "    # Reset tracking\n",
    "    inserted_company_ids = set()\n",
    "    inserted_person_ids = set()\n",
    "    inserted_opportunity_ids = set()\n",
    "    domain_to_company_id = {}\n",
    "    email_to_person_id = {}\n",
    "    hubspot_company_to_domain = {}\n",
    "    hubspot_contact_to_email = {}\n",
    "\n",
    "    try:\n",
    "        conn = get_db_connection(TWENTY_DB_CONFIG)\n",
    "        conn.autocommit = False\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        migration_stats = {\n",
    "            'companies': {'success': 0, 'failed': 0, 'errors': []},\n",
    "            'persons': {'success': 0, 'failed': 0, 'errors': []},\n",
    "            'opportunities': {'success': 0, 'failed': 0, 'errors': []}\n",
    "        }\n",
    "\n",
    "        # PHASE 1: Migrate Companies\n",
    "        print(\"\\n\" + \"=\"*100)\n",
    "        print(\"PHASE 1: MIGRATING COMPANIES\")\n",
    "        print(\"=\"*100)\n",
    "        print(f\"Records to migrate: {len(companies_df)}\\n\")\n",
    "\n",
    "        for idx, row in companies_df.iterrows():\n",
    "            try:\n",
    "                cursor.execute(\"SAVEPOINT sp_row\")\n",
    "\n",
    "                data = build_company_insert_data(row)\n",
    "                columns = list(data.keys())\n",
    "                values = [data[col] for col in columns]\n",
    "\n",
    "                col_str = ', '.join([f'\"{col}\"' for col in columns])\n",
    "                placeholders = ', '.join(['%s'] * len(columns))\n",
    "\n",
    "                # Use conflict on unique domain column (index-backed)\n",
    "                insert_sql = f\"\"\"\n",
    "                    INSERT INTO {WORKSPACE_SCHEMA}.company ({col_str})\n",
    "                    VALUES ({placeholders})\n",
    "                    ON CONFLICT (\"domainNamePrimaryLinkUrl\") DO UPDATE SET\n",
    "                        {', '.join([f'\"{col}\" = EXCLUDED.\"{col}\"' for col in columns if col != 'id'])}\n",
    "                    RETURNING id, \"domainNamePrimaryLinkUrl\"\n",
    "                \"\"\"\n",
    "\n",
    "                cursor.execute(insert_sql, values)\n",
    "                result = cursor.fetchone()\n",
    "                if result:\n",
    "                    actual_id = result[0]\n",
    "                    actual_domain = result[1]\n",
    "\n",
    "                    migration_stats['companies']['success'] += 1\n",
    "                    inserted_company_ids.add(actual_id)\n",
    "\n",
    "                    # Build FK resolution map: domain â†’ actual company ID\n",
    "                    if actual_domain:\n",
    "                        domain_to_company_id[actual_domain] = actual_id\n",
    "\n",
    "                if (idx + 1) % 100 == 0:\n",
    "                    print(f\"  âœ… Progress: {idx+1}/{len(companies_df)}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                cursor.execute(\"ROLLBACK TO SAVEPOINT sp_row\")\n",
    "                migration_stats['companies']['failed'] += 1\n",
    "                error_msg = f\"Row {idx}: {str(e)[:150]}\"\n",
    "                migration_stats['companies']['errors'].append(error_msg)\n",
    "                if migration_stats['companies']['failed'] <= 5:\n",
    "                    print(f\"  âŒ {error_msg}\")\n",
    "\n",
    "        conn.commit()\n",
    "        print(f\"\\nâœ… Companies: {migration_stats['companies']['success']} migrated, {migration_stats['companies']['failed']} failed\")\n",
    "        print(f\"   FK Resolution Map: {len(domain_to_company_id)} domains â†’ company IDs\")\n",
    "\n",
    "        # PHASE 2: Migrate Contacts\n",
    "        print(\"\\n\" + \"=\"*100)\n",
    "        print(\"PHASE 2: MIGRATING CONTACTS\")\n",
    "        print(\"=\"*100)\n",
    "        print(f\"Records to migrate: {len(contacts_df)}\\n\")\n",
    "\n",
    "        for idx, row in contacts_df.iterrows():\n",
    "            try:\n",
    "                cursor.execute(\"SAVEPOINT sp_row\")\n",
    "\n",
    "                data = build_person_insert_data(row, cursor)\n",
    "                columns = list(data.keys())\n",
    "                values = [data[col] for col in columns]\n",
    "\n",
    "                col_str = ', '.join([f'\"{col}\"' for col in columns])\n",
    "                placeholders = ', '.join(['%s'] * len(columns))\n",
    "\n",
    "                # Use conflict on unique email column (index-backed)\n",
    "                insert_sql = f\"\"\"\n",
    "                    INSERT INTO {WORKSPACE_SCHEMA}.person ({col_str})\n",
    "                    VALUES ({placeholders})\n",
    "                    ON CONFLICT (\"emailsPrimaryEmail\") DO UPDATE SET\n",
    "                        {', '.join([f'\"{col}\" = EXCLUDED.\"{col}\"' for col in columns if col != 'id'])}\n",
    "                    RETURNING id, \"emailsPrimaryEmail\"\n",
    "                \"\"\"\n",
    "\n",
    "                cursor.execute(insert_sql, values)\n",
    "                result = cursor.fetchone()\n",
    "                if result:\n",
    "                    actual_id = result[0]\n",
    "                    actual_email = result[1]\n",
    "\n",
    "                    migration_stats['persons']['success'] += 1\n",
    "                    inserted_person_ids.add(actual_id)\n",
    "\n",
    "                    # Build FK resolution map: email â†’ actual person ID\n",
    "                    if actual_email:\n",
    "                        email_to_person_id[actual_email] = actual_id\n",
    "\n",
    "                if (idx + 1) % 100 == 0:\n",
    "                    print(f\"  âœ… Progress: {idx+1}/{len(contacts_df)}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                cursor.execute(\"ROLLBACK TO SAVEPOINT sp_row\")\n",
    "                migration_stats['persons']['failed'] += 1\n",
    "                error_msg = f\"Row {idx}: {str(e)[:150]}\"\n",
    "                migration_stats['persons']['errors'].append(error_msg)\n",
    "                if migration_stats['persons']['failed'] <= 5:\n",
    "                    print(f\"  âŒ {error_msg}\")\n",
    "\n",
    "        conn.commit()\n",
    "        print(f\"\\nâœ… Persons: {migration_stats['persons']['success']} migrated, {migration_stats['persons']['failed']} failed\")\n",
    "        print(f\"   FK Resolution Map: {len(email_to_person_id)} emails â†’ person IDs\")\n",
    "\n",
    "        # PHASE 3: Migrate Deals\n",
    "        print(\"\\n\" + \"=\"*100)\n",
    "        print(\"PHASE 3: MIGRATING DEALS\")\n",
    "        print(\"=\"*100)\n",
    "        print(f\"Records to migrate: {len(deals_df)}\\n\")\n",
    "\n",
    "        for idx, row in deals_df.iterrows():\n",
    "            try:\n",
    "                cursor.execute(\"SAVEPOINT sp_row\")\n",
    "\n",
    "                data = build_opportunity_insert_data(row, cursor)\n",
    "                columns = list(data.keys())\n",
    "                values = [data[col] for col in columns]\n",
    "\n",
    "                col_str = ', '.join([f'\"{col}\"' for col in columns])\n",
    "                placeholders = ', '.join(['%s'] * len(columns))\n",
    "\n",
    "                insert_sql = f\"\"\"\n",
    "                    INSERT INTO {WORKSPACE_SCHEMA}.opportunity ({col_str})\n",
    "                    VALUES ({placeholders})\n",
    "                    ON CONFLICT (id) DO UPDATE SET\n",
    "                        {', '.join([f'\"{col}\" = EXCLUDED.\"{col}\"' for col in columns if col != 'id'])}\n",
    "                \"\"\"\n",
    "\n",
    "                cursor.execute(insert_sql, values)\n",
    "                migration_stats['opportunities']['success'] += 1\n",
    "                inserted_opportunity_ids.add(data['id'])\n",
    "\n",
    "                if (idx + 1) % 100 == 0:\n",
    "                    print(f\"  âœ… Progress: {idx+1}/{len(deals_df)}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                cursor.execute(\"ROLLBACK TO SAVEPOINT sp_row\")\n",
    "                migration_stats['opportunities']['failed'] += 1\n",
    "                error_msg = f\"Row {idx}: {str(e)[:150]}\"\n",
    "                migration_stats['opportunities']['errors'].append(error_msg)\n",
    "                if migration_stats['opportunities']['failed'] <= 5:\n",
    "                    print(f\"  âŒ {error_msg}\")\n",
    "\n",
    "        conn.commit()\n",
    "        print(f\"\\nâœ… Opportunities: {migration_stats['opportunities']['success']} migrated, {migration_stats['opportunities']['failed']} failed\")\n",
    "\n",
    "        # Summary\n",
    "        print(\"\\n\" + \"=\"*100)\n",
    "        print(\"ðŸŽ‰ MIGRATION COMPLETE!\")\n",
    "        print(\"=\"*100)\n",
    "\n",
    "        total_success = sum([migration_stats[k]['success'] for k in migration_stats])\n",
    "        total_failed = sum([migration_stats[k]['failed'] for k in migration_stats])\n",
    "\n",
    "        print(f\"\"\"\n",
    "SUMMARY:\n",
    "  Companies:     {migration_stats['companies']['success']:5d} âœ…  {migration_stats['companies']['failed']:5d} âŒ\n",
    "  Persons:       {migration_stats['persons']['success']:5d} âœ…  {migration_stats['persons']['failed']:5d} âŒ\n",
    "  Opportunities: {migration_stats['opportunities']['success']:5d} âœ…  {migration_stats['opportunities']['failed']:5d} âŒ\n",
    "\n",
    "  TOTAL:         {total_success:5d} âœ…  {total_failed:5d} âŒ\n",
    "  Success Rate:  {(total_success/(total_success+total_failed)*100) if (total_success+total_failed) > 0 else 0:.1f}%\n",
    "        \"\"\")\n",
    "\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "\n",
    "        # return stats and ids for cleanup\n",
    "        return migration_stats, {\n",
    "            'company_ids': list(inserted_company_ids),\n",
    "            'person_ids': list(inserted_person_ids),\n",
    "            'opportunity_ids': list(inserted_opportunity_ids)\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ FATAL ERROR: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        if 'conn' in locals() and conn:\n",
    "            conn.rollback()\n",
    "            conn.close()\n",
    "        return None, None\n",
    "\n",
    "# Execute the migration\n",
    "print(\"\\nðŸš€ STARTING MIGRATION...\")\n",
    "result = execute_migration()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb2ab45",
   "metadata": {},
   "source": [
    "## 6. Verify Migration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1a6a32e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "VERIFYING MIGRATED DATA\n",
      "====================================================================================================\n",
      "âœ“ Database connection established\n",
      "\n",
      "ðŸ“Š Record counts:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "  Companies:     14\n",
      "  Persons:       10\n",
      "  Opportunities: 12\n",
      "\n",
      "\n",
      "ðŸ¢ Sample Company:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "  ID: c28e47b5-f09d-5867-bb9d-de0ea8e924f9\n",
      "  Name: Stadt Karlsruhe\n",
      "  Domain: wifoe.karlsruhe.de\n",
      "  Employees: None\n",
      "  City: None\n",
      "  Created: 2025-10-10 08:11:00+00:00\n",
      "\n",
      "\n",
      "ðŸ‘¤ Sample Person (with company):\n",
      "----------------------------------------------------------------------------------------------------\n",
      "  ID: fdf01828-b6f8-57f7-9c75-c40baf6a8ef5\n",
      "  Name: Dario Arndt\n",
      "  Email: arndt@luebeck.org\n",
      "  Company ID: d97685a9-3420-5313-a01b-52011550eab6\n",
      "  Company Name: WirtschaftsfÃ¶rderung LÃ¼beck\n",
      "\n",
      "\n",
      "ðŸ’¼ Sample Opportunity (with relationships):\n",
      "----------------------------------------------------------------------------------------------------\n",
      "  ID: a5360ed7-5e97-503a-8b64-11f2a3d657c3\n",
      "  Name: WirtschaftsfÃ¶rderung Karlsruhe\n",
      "  Amount: $10000.00 USD\n",
      "  Stage: EXPLORATION_MEETING_SCHEDULED\n",
      "  Company: WirtschaftsfÃ¶rderung Karlsruhe\n",
      "  Contact: Diethelm Rumpel\n",
      "\n",
      "\n",
      "ðŸ”— Relationship Statistics:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "  Persons with company: 9/10 (90.0%)\n",
      "  Opportunities with company: 10/12 (83.3%)\n",
      "  Opportunities with contact: 10/12 (83.3%)\n",
      "\n",
      "====================================================================================================\n",
      "âœ… VERIFICATION COMPLETE!\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Verify migrated data\n",
    "print(\"=\"*100)\n",
    "print(\"VERIFYING MIGRATED DATA\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "try:\n",
    "    conn = get_db_connection(TWENTY_DB_CONFIG)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Count records\n",
    "    print(\"\\nðŸ“Š Record counts:\")\n",
    "    print(\"-\"*100)\n",
    "\n",
    "    cursor.execute(f\"SELECT COUNT(*) FROM {WORKSPACE_SCHEMA}.company\")\n",
    "    company_count = cursor.fetchone()[0]\n",
    "    print(f\"  Companies:     {company_count}\")\n",
    "\n",
    "    cursor.execute(f\"SELECT COUNT(*) FROM {WORKSPACE_SCHEMA}.person\")\n",
    "    person_count = cursor.fetchone()[0]\n",
    "    print(f\"  Persons:       {person_count}\")\n",
    "\n",
    "    cursor.execute(f\"SELECT COUNT(*) FROM {WORKSPACE_SCHEMA}.opportunity\")\n",
    "    opportunity_count = cursor.fetchone()[0]\n",
    "    print(f\"  Opportunities: {opportunity_count}\")\n",
    "\n",
    "    # Sample company\n",
    "    print(\"\\n\\nðŸ¢ Sample Company:\")\n",
    "    print(\"-\"*100)\n",
    "    cursor.execute(f\"\"\"\n",
    "        SELECT id, name, \"domainNamePrimaryLinkUrl\", employees, \"addressAddressCity\", \"createdAt\"\n",
    "        FROM {WORKSPACE_SCHEMA}.company\n",
    "        WHERE name IS NOT NULL\n",
    "        ORDER BY \"createdAt\" DESC\n",
    "        LIMIT 1\n",
    "    \"\"\")\n",
    "    sample = cursor.fetchone()\n",
    "    if sample:\n",
    "        print(f\"  ID: {sample[0]}\")\n",
    "        print(f\"  Name: {sample[1]}\")\n",
    "        print(f\"  Domain: {sample[2]}\")\n",
    "        print(f\"  Employees: {sample[3]}\")\n",
    "        print(f\"  City: {sample[4]}\")\n",
    "        print(f\"  Created: {sample[5]}\")\n",
    "\n",
    "    # Sample person with company\n",
    "    print(\"\\n\\nðŸ‘¤ Sample Person (with company):\")\n",
    "    print(\"-\"*100)\n",
    "    cursor.execute(f\"\"\"\n",
    "        SELECT\n",
    "            p.id,\n",
    "            p.\"nameFirstName\",\n",
    "            p.\"nameLastName\",\n",
    "            p.\"emailsPrimaryEmail\",\n",
    "            p.\"companyId\",\n",
    "            c.name as company_name\n",
    "        FROM {WORKSPACE_SCHEMA}.person p\n",
    "        LEFT JOIN {WORKSPACE_SCHEMA}.company c ON p.\"companyId\" = c.id\n",
    "        WHERE p.\"nameFirstName\" IS NOT NULL AND p.\"companyId\" IS NOT NULL\n",
    "        ORDER BY p.\"createdAt\" DESC\n",
    "        LIMIT 1\n",
    "    \"\"\")\n",
    "    sample = cursor.fetchone()\n",
    "    if sample:\n",
    "        print(f\"  ID: {sample[0]}\")\n",
    "        print(f\"  Name: {sample[1]} {sample[2]}\")\n",
    "        print(f\"  Email: {sample[3]}\")\n",
    "        print(f\"  Company ID: {sample[4]}\")\n",
    "        print(f\"  Company Name: {sample[5]}\")\n",
    "\n",
    "    # Sample opportunity with relationships\n",
    "    print(\"\\n\\nðŸ’¼ Sample Opportunity (with relationships):\")\n",
    "    print(\"-\"*100)\n",
    "    cursor.execute(f\"\"\"\n",
    "        SELECT\n",
    "            o.id,\n",
    "            o.name,\n",
    "            o.\"amountAmountMicros\",\n",
    "            o.\"amountCurrencyCode\",\n",
    "            o.stage,\n",
    "            c.name as company_name,\n",
    "            p.\"nameFirstName\" || ' ' || p.\"nameLastName\" as contact_name\n",
    "        FROM {WORKSPACE_SCHEMA}.opportunity o\n",
    "        LEFT JOIN {WORKSPACE_SCHEMA}.company c ON o.\"companyId\" = c.id\n",
    "        LEFT JOIN {WORKSPACE_SCHEMA}.person p ON o.\"pointOfContactId\" = p.id\n",
    "        WHERE o.name IS NOT NULL\n",
    "        ORDER BY o.\"createdAt\" DESC\n",
    "        LIMIT 1\n",
    "    \"\"\")\n",
    "    sample = cursor.fetchone()\n",
    "    if sample:\n",
    "        amount_display = f\"${sample[2]/1000000:.2f} {sample[3]}\" if sample[2] else \"N/A\"\n",
    "        print(f\"  ID: {sample[0]}\")\n",
    "        print(f\"  Name: {sample[1]}\")\n",
    "        print(f\"  Amount: {amount_display}\")\n",
    "        print(f\"  Stage: {sample[4]}\")\n",
    "        print(f\"  Company: {sample[5]}\")\n",
    "        print(f\"  Contact: {sample[6]}\")\n",
    "\n",
    "    # Relationship stats\n",
    "    print(\"\\n\\nðŸ”— Relationship Statistics:\")\n",
    "    print(\"-\"*100)\n",
    "\n",
    "    cursor.execute(f'SELECT COUNT(*) FROM {WORKSPACE_SCHEMA}.person WHERE \"companyId\" IS NOT NULL')\n",
    "    persons_with_company = cursor.fetchone()[0]\n",
    "    print(f\"  Persons with company: {persons_with_company}/{person_count} ({persons_with_company/person_count*100:.1f}%)\")\n",
    "\n",
    "    cursor.execute(f'SELECT COUNT(*) FROM {WORKSPACE_SCHEMA}.opportunity WHERE \"companyId\" IS NOT NULL')\n",
    "    opps_with_company = cursor.fetchone()[0]\n",
    "    print(f\"  Opportunities with company: {opps_with_company}/{opportunity_count} ({opps_with_company/opportunity_count*100:.1f}%)\")\n",
    "\n",
    "    cursor.execute(f'SELECT COUNT(*) FROM {WORKSPACE_SCHEMA}.opportunity WHERE \"pointOfContactId\" IS NOT NULL')\n",
    "    opps_with_contact = cursor.fetchone()[0]\n",
    "    print(f\"  Opportunities with contact: {opps_with_contact}/{opportunity_count} ({opps_with_contact/opportunity_count*100:.1f}%)\")\n",
    "\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"âœ… VERIFICATION COMPLETE!\")\n",
    "    print(\"=\"*100)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ Error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbc39f2",
   "metadata": {},
   "source": [
    "## 7. ðŸ§¹ Cleanup - Purge Test Data (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870b2834",
   "metadata": {},
   "source": [
    "# âš ï¸  DANGER ZONE: Purge all migrated data from tables\n",
    "# Use this cell to clean up test data before running another migration\n",
    "print(\"\\nðŸ§¹ PURGING ALL DATA FROM TABLES...\")\n",
    "print(\"=\"*100)\n",
    "print(\"âš ï¸  WARNING: This will delete ALL records from company, person, and opportunity tables!\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "try:\n",
    "    conn = get_db_connection(TWENTY_DB_CONFIG)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Delete in correct order (FK constraints)\n",
    "    print(\"\\nDeleting opportunities...\")\n",
    "    cursor.execute(f\"DELETE FROM {WORKSPACE_SCHEMA}.opportunity\")\n",
    "    opp_deleted = cursor.rowcount\n",
    "\n",
    "    print(\"Deleting persons...\")\n",
    "    cursor.execute(f\"DELETE FROM {WORKSPACE_SCHEMA}.person\")\n",
    "    person_deleted = cursor.rowcount\n",
    "\n",
    "    print(\"Deleting companies...\")\n",
    "    cursor.execute(f\"DELETE FROM {WORKSPACE_SCHEMA}.company\")\n",
    "    company_deleted = cursor.rowcount\n",
    "\n",
    "    conn.commit()\n",
    "\n",
    "    # Verify empty tables\n",
    "    cursor.execute(f\"SELECT COUNT(*) FROM {WORKSPACE_SCHEMA}.company\")\n",
    "    c_count = cursor.fetchone()[0]\n",
    "    cursor.execute(f\"SELECT COUNT(*) FROM {WORKSPACE_SCHEMA}.person\")\n",
    "    p_count = cursor.fetchone()[0]\n",
    "    cursor.execute(f\"SELECT COUNT(*) FROM {WORKSPACE_SCHEMA}.opportunity\")\n",
    "    o_count = cursor.fetchone()[0]\n",
    "\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "\n",
    "    print(f\"\\nâœ… PURGE COMPLETE:\")\n",
    "    print(f\"   Companies deleted:     {company_deleted} (remaining: {c_count})\")\n",
    "    print(f\"   Persons deleted:       {person_deleted} (remaining: {p_count})\")\n",
    "    print(f\"   Opportunities deleted: {opp_deleted} (remaining: {o_count})\")\n",
    "    print(\"=\"*100)\n",
    "    print(\"âœ… All tables are now empty and ready for a fresh migration!\")\n",
    "    print(\"=\"*100)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ Error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
